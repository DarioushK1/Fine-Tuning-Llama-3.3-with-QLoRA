{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs: 4\n",
      "GPU 0: NVIDIA GeForce RTX 3090\n",
      "GPU 1: NVIDIA GeForce RTX 3090\n",
      "GPU 2: NVIDIA GeForce RTX 4090\n",
      "GPU 3: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "775fb196bf1c4969817120f57a9a372c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "\n",
      "Current device map:\n",
      "model.embed_tokens: 0\n",
      "model.layers.0: 0\n",
      "model.layers.1: 0\n",
      "model.layers.2: 0\n",
      "model.layers.3: 0\n",
      "model.layers.4: 0\n",
      "model.layers.5: 0\n",
      "model.layers.6: 0\n",
      "model.layers.7: 0\n",
      "model.layers.8: 0\n",
      "model.layers.9: 0\n",
      "model.layers.10: 0\n",
      "model.layers.11: 0\n",
      "model.layers.12: 0\n",
      "model.layers.13: 0\n",
      "model.layers.14: 0\n",
      "model.layers.15: 0\n",
      "model.layers.16: 0\n",
      "model.layers.17: 0\n",
      "model.layers.18: 0\n",
      "model.layers.19: 0\n",
      "model.layers.20: 0\n",
      "model.layers.21: 0\n",
      "model.layers.22: 0\n",
      "model.layers.23: 0\n",
      "model.layers.24: 0\n",
      "model.layers.25: 0\n",
      "model.layers.26: 0\n",
      "model.layers.27: 0\n",
      "model.layers.28: 0\n",
      "model.layers.29: 0\n",
      "model.layers.30: 0\n",
      "model.layers.31: 0\n",
      "model.layers.32: 1\n",
      "model.layers.33: 1\n",
      "model.layers.34: 1\n",
      "model.layers.35: 1\n",
      "model.layers.36: 1\n",
      "model.layers.37: 1\n",
      "model.layers.38: 1\n",
      "model.layers.39: 1\n",
      "model.layers.40: 1\n",
      "model.layers.41: 1\n",
      "model.layers.42: 1\n",
      "model.layers.43: 1\n",
      "model.layers.44: 1\n",
      "model.layers.45: 1\n",
      "model.layers.46: 1\n",
      "model.layers.47: 1\n",
      "model.layers.48: 1\n",
      "model.layers.49: 1\n",
      "model.layers.50: 1\n",
      "model.layers.51: 1\n",
      "model.layers.52: 1\n",
      "model.layers.53: 1\n",
      "model.layers.54: 1\n",
      "model.layers.55: 1\n",
      "model.layers.56: 1\n",
      "model.layers.57: 1\n",
      "model.layers.58: 1\n",
      "model.layers.59: 1\n",
      "model.layers.60: 1\n",
      "model.layers.61: 1\n",
      "model.layers.62: 1\n",
      "model.layers.63: 1\n",
      "model.layers.64: 1\n",
      "model.layers.65: 1\n",
      "model.layers.66: 1\n",
      "model.layers.67: 1\n",
      "model.layers.68: 1\n",
      "model.layers.69: 1\n",
      "model.layers.70: 2\n",
      "model.layers.71: 2\n",
      "model.layers.72: 2\n",
      "model.layers.73: 2\n",
      "model.layers.74: 2\n",
      "model.layers.75: 2\n",
      "model.layers.76: 2\n",
      "model.layers.77: 2\n",
      "model.layers.78: 2\n",
      "model.layers.79: 2\n",
      "model.norm: 2\n",
      "model.rotary_emb: 2\n",
      "lm_head: 2\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    " \n",
    "import bitsandbytes as bnb\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from accelerate import load_checkpoint_and_dispatch, infer_auto_device_map\n",
    "import torch\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512,expandable_segments:True'\n",
    "# 1. Free up GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 2. Configure quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True  # Add this line\n",
    ")\n",
    "\n",
    "# 3. Model info\n",
    "model_id = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "local_model_dir = \"./llama3_70b_checkpoint\"\n",
    "\n",
    "# 4. Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 5. Calculate max memory for each GPU\n",
    "n_gpus = torch.cuda.device_count()\n",
    "# max_memory = {\n",
    "#     0: \"17GiB\",\n",
    "#     1: \"17GiB\",\n",
    "#     2: \"0GiB\",  # Add this with minimal memory\n",
    "#     3: \"19GiB\"\n",
    "\n",
    "# }\n",
    "max_memory = {\n",
    "    0: \"17GiB\",\n",
    "    1: \"17GiB\",\n",
    "    2: \"19GiB\"  # Add this with minimal memory\n",
    "\n",
    "}\n",
    "# 6. Load model with device map inference\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"sequential\",  # Let transformers handle device mapping\n",
    "    max_memory=max_memory,\n",
    "    trust_remote_code = True \n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(\"\\nCurrent device map:\")\n",
    "for name, device in model.hf_device_map.items():\n",
    "    print(f\"{name}: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 89128960 / 36417314816 (0.24%)\n"
     ]
    }
   ],
   "source": [
    "# Configure LoRA\n",
    "lora_config =  LoraConfig(\n",
    "    r=32, #Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        'q_proj',\n",
    "        'k_proj',\n",
    "        'v_proj',\n",
    "        'dense'\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05  # Conventional\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "# Function to print trainable parameters\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    total_params = 0\n",
    "    for param in model.parameters():\n",
    "        total_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"Trainable parameters: {trainable_params} / {total_params} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "\n",
    "# Print the trainable parameters\n",
    "print_trainable_parameters(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d88b116b23a4ae1b79b3c870b9485de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9952 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['number', 'title', 'problem_statement', 'solution', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 9952\n",
      "})\n",
      "{'input_ids': tensor([128000,  10174,   7282,  ..., 128009, 128009, 128009]), 'attention_mask': tensor([1, 1, 1,  ..., 0, 0, 0]), 'labels': tensor([  -100,   -100,   -100,  ..., 128009, 128009, 128009])}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset, Features, Sequence, Value\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def generate_and_tokenize_prompt(examples):\n",
    "    prompts = []\n",
    "    solutions = []\n",
    "    \n",
    "    for title, problem, solution in zip(examples['title'], examples['problem_statement'], examples['solution']):\n",
    "        prompt = f\"\"\"<human>: Below is a control systems engineering problem. Solve it step by step.\n",
    "        \n",
    "Title: {title}\n",
    "Problem: {problem}\n",
    "\n",
    "<assistant>:\"\"\".strip()\n",
    "        \n",
    "        prompts.append(prompt)\n",
    "        solutions.append(solution)\n",
    "\n",
    "    # Tokenize prompts and solutions\n",
    "    tokenized_inputs = tokenizer(prompts, padding='max_length', truncation=True, max_length=2048)\n",
    "    tokenized_labels = tokenizer(solutions, padding='max_length', truncation=True, max_length=2048)\n",
    "\n",
    "    # Convert lists to tensors first\n",
    "    input_ids = torch.tensor(tokenized_inputs['input_ids'])\n",
    "    labels = torch.full_like(input_ids, -100)\n",
    "    \n",
    "    # Get prompt lengths\n",
    "    prompt_lens = [len(tokenizer(p)['input_ids']) for p in prompts]\n",
    "    \n",
    "    # Fill in the solution labels\n",
    "    for i, prompt_len in enumerate(prompt_lens):\n",
    "        # Convert to tensor before assigning\n",
    "        solution_ids = torch.tensor(tokenized_labels['input_ids'][i][:(2048-prompt_len)])\n",
    "        labels[i, prompt_len:prompt_len + len(solution_ids)] = solution_ids\n",
    "\n",
    "    # Return dictionary with all required fields\n",
    "    return {\n",
    "        'input_ids': tokenized_inputs['input_ids'],\n",
    "        'attention_mask': tokenized_inputs['attention_mask'],\n",
    "        'labels': labels.tolist()  # Convert back to list for dataset\n",
    "    }\n",
    "\n",
    "# Load and preprocess function\n",
    "def load_and_preprocess_data(file_path):\n",
    "    # Load the raw JSON data\n",
    "    with open(file_path, \"r\") as f:\n",
    "        raw_data = json.load(f)[\"problems\"]\n",
    "    \n",
    "    # Convert to Dataset\n",
    "    dataset = Dataset.from_list(raw_data)\n",
    "    \n",
    "    # Process data\n",
    "    processed_data = dataset.map(\n",
    "        generate_and_tokenize_prompt,\n",
    "        batched=True,\n",
    "        batch_size=32,\n",
    "        load_from_cache_file=False\n",
    "    )\n",
    "    \n",
    "    # Set format\n",
    "    processed_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    \n",
    "    return processed_data\n",
    "# Main execution\n",
    "file_path = \"generated_problems_ControlBench_o1.json\"\n",
    "data = load_and_preprocess_data(file_path)\n",
    "# Verify\n",
    "print(data)\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"experiments\"\n",
    " \n",
    "training_args = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=1,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_steps=80,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    report_to=\"tensorboard\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dk12/miniconda3/envs/llama3_finetuning_env/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [80/80 1:37:14, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.022700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.726600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.806400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.017000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.856600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.149300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.894000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.924800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.577200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.373900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.356500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.179200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.061200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.734400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.796900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.682800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.610200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.668600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.668800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.666700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.690300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.552800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.562200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.618600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.608900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.523500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.625400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.551200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.525400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.667400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.559100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.562100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.438500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.555700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.490300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.648600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.508300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.516000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.576100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.572500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.543000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.510800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.556500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.528900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.441300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.552200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.476000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.389800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.448600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.422400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.568300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.532200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.467300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.570100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.408200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.481500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.356800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.467900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.544800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.585300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.489600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.405700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.464100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.477300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.436100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.527500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.538000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.567300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.601600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.415800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.579100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.540200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.525100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.505300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.399900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.452000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.412600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.492500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.353700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.429700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=80, training_loss=0.7198976818472147, metrics={'train_runtime': 5908.4294, 'train_samples_per_second': 0.054, 'train_steps_per_second': 0.014, 'total_flos': 2.7364751692529664e+17, 'train_loss': 0.7198976818472147, 'epoch': 0.03215434083601286})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data,\n",
    "    args=training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"trained-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "367fba9c950c453f9e35f207768c5506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m config \u001b[38;5;241m=\u001b[39m PeftConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(PEFT_MODEL)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load the base model\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Base model path from the PEFT config\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Load the tokenizer\u001b[39;00m\n\u001b[1;32m     16\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(config\u001b[38;5;241m.\u001b[39mbase_model_name_or_path)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3_finetuning_env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3_finetuning_env/lib/python3.11/site-packages/transformers/modeling_utils.py:4264\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4254\u001b[0m         load_contexts\u001b[38;5;241m.\u001b[39mappend(tp_device)\n\u001b[1;32m   4256\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(load_contexts):\n\u001b[1;32m   4257\u001b[0m         (\n\u001b[1;32m   4258\u001b[0m             model,\n\u001b[1;32m   4259\u001b[0m             missing_keys,\n\u001b[1;32m   4260\u001b[0m             unexpected_keys,\n\u001b[1;32m   4261\u001b[0m             mismatched_keys,\n\u001b[1;32m   4262\u001b[0m             offload_index,\n\u001b[1;32m   4263\u001b[0m             error_msgs,\n\u001b[0;32m-> 4264\u001b[0m         ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4265\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4266\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4267\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   4268\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4269\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4270\u001b[0m \u001b[43m            \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4271\u001b[0m \u001b[43m            \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4272\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4273\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4274\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4275\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4276\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4277\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4278\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4279\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4280\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4281\u001b[0m \u001b[43m            \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4282\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4284\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   4285\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3_finetuning_env/lib/python3.11/site-packages/transformers/modeling_utils.py:4777\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)\u001b[0m\n\u001b[1;32m   4773\u001b[0m                 set_module_tensor_to_device(\n\u001b[1;32m   4774\u001b[0m                     model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   4775\u001b[0m                 )\n\u001b[1;32m   4776\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4777\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4778\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4779\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4780\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4781\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4782\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4783\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4784\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4785\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4786\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4791\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4792\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4793\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4794\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4795\u001b[0m     \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3_finetuning_env/lib/python3.11/site-packages/transformers/modeling_utils.py:905\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m old_param \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 905\u001b[0m         param \u001b[38;5;241m=\u001b[39m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mold_param\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m old_param\u001b[38;5;241m.\u001b[39mis_contiguous():\n\u001b[1;32m    908\u001b[0m         param \u001b[38;5;241m=\u001b[39m param\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Replace \"trained-model\" with the directory where your model is saved\n",
    "PEFT_MODEL = \"trained-model\"\n",
    "\n",
    "# Load the PEFT configuration\n",
    "config = PeftConfig.from_pretrained(PEFT_MODEL)\n",
    "\n",
    "# Load the base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,  # Base model path from the PEFT config\n",
    "    return_dict=True,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the PEFT model\n",
    "model = PeftModel.from_pretrained(model, PEFT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b91f9692b00641af888514bd2a98d472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9952 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. First, install required additional packages if not already installed\n",
    "\n",
    "# 2. Import additional required libraries\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "import json\n",
    "\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    formatted_data = []\n",
    "    for problem in data['problems']:\n",
    "        prompt = (\n",
    "            f\"Below is a control systems engineering problem. Solve it step by step.\\n\\n\"\n",
    "            f\"Title: {problem['title']}\\n\"\n",
    "            f\"Problem: {problem['problem_statement']}\\n\"\n",
    "            f\"Solution:\"\n",
    "        )\n",
    "        completion = problem['solution']\n",
    "\n",
    "        # Store raw text in \"text\" instead of \"input_ids\"\n",
    "        formatted_text = {\n",
    "            \"text\": f\"{prompt}\\n{completion}</s>\",\n",
    "            \"title\": problem['title'],\n",
    "            \"number\": problem.get('number', None)\n",
    "        }\n",
    "        formatted_data.append(formatted_text)\n",
    "\n",
    "    return formatted_data\n",
    "\n",
    "train_data = load_data(\"generated_problems_ControlBench_o1.json\")\n",
    "dataset = Dataset.from_list(train_data)\n",
    "\n",
    "# 1. Tokenize\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, max_length=1024)\n",
    "\n",
    "# 2. Apply to entire dataset\n",
    "dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dk12/miniconda3/envs/llama3_finetuning_env/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from tensorboard import notebook\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class ConsoleLossCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None and \"loss\" in logs:\n",
    "            print(f\"Step {state.global_step} - Loss: {logs['loss']}\")\n",
    "\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=64,                # Even more capacity\n",
    "    lora_alpha=128,       # Matched with higher rank\n",
    "    target_modules=[     # All major modules\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"\n",
    "\n",
    "    ],\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "# Prepare the model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(\"./control_system_tuned_model\", exist_ok=True)\n",
    "\n",
    "# Set up training arguments with TensorBoard logging\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./control_system_tuned_model\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    save_steps=500,\n",
    "    eval_steps =500,\n",
    "    logging_steps=1,  # Changed from 10 to 1 to log more frequently\n",
    "    logging_first_step=True,  # Add this to log the first step\n",
    "    logging_nan_inf_filter=False,  # Add this to show all loss values\n",
    "    learning_rate=5e-6,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=1.0,\n",
    "    warmup_ratio=0.1,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    logging_dir=\"./control_system_tuned_model/runs\",\n",
    "    disable_tqdm=False,  # Enables progress bar and logs\n",
    "    gradient_checkpointing=True \n",
    ")\n",
    "\n",
    "# Split dataset into train and validation\n",
    "split_dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_data = split_dataset[\"train\"]\n",
    "eval_data  = split_dataset[\"test\"]\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    callbacks=[ConsoleLossCallback()],  # Add the callback here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 286474), started 20:06:41 ago. (Use '!kill 286474' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Cell 1: Start TensorBoard in a separate cell\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./control_system_tuned_model/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='6717' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 501/6717 3:22:36 < 42:03:57, 0.04 it/s, Epoch 0.22/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 - Loss: 0.59\n",
      "Step 2 - Loss: 0.5206\n",
      "Step 3 - Loss: 0.7153\n",
      "Step 4 - Loss: 0.6475\n",
      "Step 5 - Loss: 0.7831\n",
      "Step 6 - Loss: 0.7622\n",
      "Step 7 - Loss: 0.8032\n",
      "Step 8 - Loss: 0.7077\n",
      "Step 9 - Loss: 0.789\n",
      "Step 10 - Loss: 0.8429\n",
      "Step 11 - Loss: 0.8321\n",
      "Step 12 - Loss: 0.7997\n",
      "Step 13 - Loss: 0.7496\n",
      "Step 14 - Loss: 0.6279\n",
      "Step 15 - Loss: 0.7245\n",
      "Step 16 - Loss: 0.9579\n",
      "Step 17 - Loss: 0.7972\n",
      "Step 18 - Loss: 0.9679\n",
      "Step 19 - Loss: 0.7951\n",
      "Step 20 - Loss: 0.7273\n",
      "Step 21 - Loss: 0.843\n",
      "Step 22 - Loss: 0.9088\n",
      "Step 23 - Loss: 0.8539\n",
      "Step 24 - Loss: 0.8979\n",
      "Step 25 - Loss: 0.7965\n",
      "Step 26 - Loss: 0.8654\n",
      "Step 27 - Loss: 0.8524\n",
      "Step 28 - Loss: 0.9925\n",
      "Step 29 - Loss: 0.8508\n",
      "Step 30 - Loss: 0.8155\n",
      "Step 31 - Loss: 0.94\n",
      "Step 32 - Loss: 0.9262\n",
      "Step 33 - Loss: 0.9008\n",
      "Step 34 - Loss: 0.9217\n",
      "Step 35 - Loss: 1.0107\n",
      "Step 36 - Loss: 1.1129\n",
      "Step 37 - Loss: 1.0977\n",
      "Step 38 - Loss: 1.1489\n",
      "Step 39 - Loss: 0.9306\n",
      "Step 40 - Loss: 1.0087\n",
      "Step 41 - Loss: 1.0219\n",
      "Step 42 - Loss: 1.0427\n",
      "Step 43 - Loss: 1.0439\n",
      "Step 44 - Loss: 1.3035\n",
      "Step 45 - Loss: 1.1195\n",
      "Step 46 - Loss: 1.1929\n",
      "Step 47 - Loss: 1.2524\n",
      "Step 48 - Loss: 1.1348\n",
      "Step 49 - Loss: 1.3244\n",
      "Step 50 - Loss: 1.7882\n",
      "Step 51 - Loss: 0.6492\n",
      "Step 52 - Loss: 0.5478\n",
      "Step 53 - Loss: 0.7763\n",
      "Step 54 - Loss: 0.5924\n",
      "Step 55 - Loss: 0.5708\n",
      "Step 56 - Loss: 0.6692\n",
      "Step 57 - Loss: 0.6421\n",
      "Step 58 - Loss: 0.6017\n",
      "Step 59 - Loss: 0.6765\n",
      "Step 60 - Loss: 0.6585\n",
      "Step 61 - Loss: 0.7818\n",
      "Step 62 - Loss: 0.7261\n",
      "Step 63 - Loss: 0.7327\n",
      "Step 64 - Loss: 0.7761\n",
      "Step 65 - Loss: 0.7553\n",
      "Step 66 - Loss: 0.8766\n",
      "Step 67 - Loss: 0.7372\n",
      "Step 68 - Loss: 0.7276\n",
      "Step 69 - Loss: 0.7654\n",
      "Step 70 - Loss: 0.7937\n",
      "Step 71 - Loss: 0.816\n",
      "Step 72 - Loss: 0.7852\n",
      "Step 73 - Loss: 0.8377\n",
      "Step 74 - Loss: 0.6988\n",
      "Step 75 - Loss: 0.8508\n",
      "Step 76 - Loss: 1.0551\n",
      "Step 77 - Loss: 0.8389\n",
      "Step 78 - Loss: 1.0338\n",
      "Step 79 - Loss: 0.9378\n",
      "Step 80 - Loss: 0.8719\n",
      "Step 81 - Loss: 0.9533\n",
      "Step 82 - Loss: 0.8907\n",
      "Step 83 - Loss: 0.849\n",
      "Step 84 - Loss: 0.9279\n",
      "Step 85 - Loss: 0.9195\n",
      "Step 86 - Loss: 0.909\n",
      "Step 87 - Loss: 0.9988\n",
      "Step 88 - Loss: 0.9488\n",
      "Step 89 - Loss: 0.9343\n",
      "Step 90 - Loss: 0.9791\n",
      "Step 91 - Loss: 1.0387\n",
      "Step 92 - Loss: 0.9943\n",
      "Step 93 - Loss: 0.9674\n",
      "Step 94 - Loss: 1.0544\n",
      "Step 95 - Loss: 1.0343\n",
      "Step 96 - Loss: 1.1011\n",
      "Step 97 - Loss: 1.1027\n",
      "Step 98 - Loss: 1.1386\n",
      "Step 99 - Loss: 1.1967\n",
      "Step 100 - Loss: 1.5871\n",
      "Step 101 - Loss: 0.5566\n",
      "Step 102 - Loss: 0.6028\n",
      "Step 103 - Loss: 0.5393\n",
      "Step 104 - Loss: 0.6201\n",
      "Step 105 - Loss: 0.6166\n",
      "Step 106 - Loss: 0.6767\n",
      "Step 107 - Loss: 0.7746\n",
      "Step 108 - Loss: 0.5864\n",
      "Step 109 - Loss: 0.6382\n",
      "Step 110 - Loss: 0.636\n",
      "Step 111 - Loss: 0.6492\n",
      "Step 112 - Loss: 0.6769\n",
      "Step 113 - Loss: 0.6425\n",
      "Step 114 - Loss: 0.7067\n",
      "Step 115 - Loss: 0.7041\n",
      "Step 116 - Loss: 0.6528\n",
      "Step 117 - Loss: 0.7429\n",
      "Step 118 - Loss: 0.7343\n",
      "Step 119 - Loss: 0.7536\n",
      "Step 120 - Loss: 0.8561\n",
      "Step 121 - Loss: 0.7271\n",
      "Step 122 - Loss: 0.7513\n",
      "Step 123 - Loss: 0.7171\n",
      "Step 124 - Loss: 0.7886\n",
      "Step 125 - Loss: 0.9458\n",
      "Step 126 - Loss: 0.8013\n",
      "Step 127 - Loss: 0.8018\n",
      "Step 128 - Loss: 0.736\n",
      "Step 129 - Loss: 0.8274\n",
      "Step 130 - Loss: 0.7733\n",
      "Step 131 - Loss: 0.8802\n",
      "Step 132 - Loss: 0.8277\n",
      "Step 133 - Loss: 0.7594\n",
      "Step 134 - Loss: 0.9713\n",
      "Step 135 - Loss: 0.8964\n",
      "Step 136 - Loss: 0.8577\n",
      "Step 137 - Loss: 0.8913\n",
      "Step 138 - Loss: 0.8905\n",
      "Step 139 - Loss: 0.8087\n",
      "Step 140 - Loss: 0.8072\n",
      "Step 141 - Loss: 0.8475\n",
      "Step 142 - Loss: 0.9293\n",
      "Step 143 - Loss: 0.9242\n",
      "Step 144 - Loss: 0.9833\n",
      "Step 145 - Loss: 0.9369\n",
      "Step 146 - Loss: 1.03\n",
      "Step 147 - Loss: 1.1235\n",
      "Step 148 - Loss: 1.006\n",
      "Step 149 - Loss: 1.1272\n",
      "Step 150 - Loss: 1.3791\n",
      "Step 151 - Loss: 0.4295\n",
      "Step 152 - Loss: 0.6042\n",
      "Step 153 - Loss: 0.5751\n",
      "Step 154 - Loss: 0.5675\n",
      "Step 155 - Loss: 0.5076\n",
      "Step 156 - Loss: 0.5384\n",
      "Step 157 - Loss: 0.5518\n",
      "Step 158 - Loss: 0.5829\n",
      "Step 159 - Loss: 0.7318\n",
      "Step 160 - Loss: 0.5834\n",
      "Step 161 - Loss: 0.5491\n",
      "Step 162 - Loss: 0.6358\n",
      "Step 163 - Loss: 0.5332\n",
      "Step 164 - Loss: 0.6589\n",
      "Step 165 - Loss: 0.5385\n",
      "Step 166 - Loss: 0.6384\n",
      "Step 167 - Loss: 0.7587\n",
      "Step 168 - Loss: 0.7182\n",
      "Step 169 - Loss: 0.7246\n",
      "Step 170 - Loss: 0.6945\n",
      "Step 171 - Loss: 0.6439\n",
      "Step 172 - Loss: 0.6784\n",
      "Step 173 - Loss: 0.6736\n",
      "Step 174 - Loss: 0.7641\n",
      "Step 175 - Loss: 0.7429\n",
      "Step 176 - Loss: 0.6606\n",
      "Step 177 - Loss: 0.699\n",
      "Step 178 - Loss: 0.7575\n",
      "Step 179 - Loss: 0.7399\n",
      "Step 180 - Loss: 0.6064\n",
      "Step 181 - Loss: 0.6739\n",
      "Step 182 - Loss: 0.7903\n",
      "Step 183 - Loss: 0.7164\n",
      "Step 184 - Loss: 0.9569\n",
      "Step 185 - Loss: 0.8618\n",
      "Step 186 - Loss: 0.7967\n",
      "Step 187 - Loss: 0.7987\n",
      "Step 188 - Loss: 0.7515\n",
      "Step 189 - Loss: 0.852\n",
      "Step 190 - Loss: 0.7577\n",
      "Step 191 - Loss: 0.6818\n",
      "Step 192 - Loss: 0.8623\n",
      "Step 193 - Loss: 0.9085\n",
      "Step 194 - Loss: 0.8304\n",
      "Step 195 - Loss: 0.8173\n",
      "Step 196 - Loss: 0.8283\n",
      "Step 197 - Loss: 0.8257\n",
      "Step 198 - Loss: 0.9032\n",
      "Step 199 - Loss: 0.9461\n",
      "Step 200 - Loss: 1.1818\n",
      "Step 201 - Loss: 0.4537\n",
      "Step 202 - Loss: 0.4162\n",
      "Step 203 - Loss: 0.5898\n",
      "Step 204 - Loss: 0.5152\n",
      "Step 205 - Loss: 0.5649\n",
      "Step 206 - Loss: 0.5176\n",
      "Step 207 - Loss: 0.534\n",
      "Step 208 - Loss: 0.5124\n",
      "Step 209 - Loss: 0.496\n",
      "Step 210 - Loss: 0.5022\n",
      "Step 211 - Loss: 0.6014\n",
      "Step 212 - Loss: 0.5355\n",
      "Step 213 - Loss: 0.4794\n",
      "Step 214 - Loss: 0.4628\n",
      "Step 215 - Loss: 0.5928\n",
      "Step 216 - Loss: 0.5973\n",
      "Step 217 - Loss: 0.6033\n",
      "Step 218 - Loss: 0.4657\n",
      "Step 219 - Loss: 0.5912\n",
      "Step 220 - Loss: 0.6498\n",
      "Step 221 - Loss: 0.5204\n",
      "Step 222 - Loss: 0.5073\n",
      "Step 223 - Loss: 0.5373\n",
      "Step 224 - Loss: 0.4987\n",
      "Step 225 - Loss: 0.5783\n",
      "Step 226 - Loss: 0.5426\n",
      "Step 227 - Loss: 0.5342\n",
      "Step 228 - Loss: 0.603\n",
      "Step 229 - Loss: 0.5478\n",
      "Step 230 - Loss: 0.6331\n",
      "Step 231 - Loss: 0.6437\n",
      "Step 232 - Loss: 0.567\n",
      "Step 233 - Loss: 0.6294\n",
      "Step 234 - Loss: 0.7285\n",
      "Step 235 - Loss: 0.5364\n",
      "Step 236 - Loss: 0.6847\n",
      "Step 237 - Loss: 0.6038\n",
      "Step 238 - Loss: 0.8092\n",
      "Step 239 - Loss: 0.5443\n",
      "Step 240 - Loss: 0.5959\n",
      "Step 241 - Loss: 0.6309\n",
      "Step 242 - Loss: 0.6781\n",
      "Step 243 - Loss: 0.7138\n",
      "Step 244 - Loss: 0.5949\n",
      "Step 245 - Loss: 0.6328\n",
      "Step 246 - Loss: 0.7177\n",
      "Step 247 - Loss: 0.6249\n",
      "Step 248 - Loss: 0.6951\n",
      "Step 249 - Loss: 0.7126\n",
      "Step 250 - Loss: 1.0455\n",
      "Step 251 - Loss: 0.4157\n",
      "Step 252 - Loss: 0.3657\n",
      "Step 253 - Loss: 0.3894\n",
      "Step 254 - Loss: 0.4751\n",
      "Step 255 - Loss: 0.5103\n",
      "Step 256 - Loss: 0.4489\n",
      "Step 257 - Loss: 0.4714\n",
      "Step 258 - Loss: 0.5144\n",
      "Step 259 - Loss: 0.4829\n",
      "Step 260 - Loss: 0.4678\n",
      "Step 261 - Loss: 0.5726\n",
      "Step 262 - Loss: 0.4457\n",
      "Step 263 - Loss: 0.4409\n",
      "Step 264 - Loss: 0.4559\n",
      "Step 265 - Loss: 0.4598\n",
      "Step 266 - Loss: 0.5006\n",
      "Step 267 - Loss: 0.46\n",
      "Step 268 - Loss: 0.4525\n",
      "Step 269 - Loss: 0.5764\n",
      "Step 270 - Loss: 0.4394\n",
      "Step 271 - Loss: 0.556\n",
      "Step 272 - Loss: 0.4912\n",
      "Step 273 - Loss: 0.4242\n",
      "Step 274 - Loss: 0.4822\n",
      "Step 275 - Loss: 0.4672\n",
      "Step 276 - Loss: 0.4775\n",
      "Step 277 - Loss: 0.4888\n",
      "Step 278 - Loss: 0.5212\n",
      "Step 279 - Loss: 0.5841\n",
      "Step 280 - Loss: 0.5239\n",
      "Step 281 - Loss: 0.5574\n",
      "Step 282 - Loss: 0.476\n",
      "Step 283 - Loss: 0.5021\n",
      "Step 284 - Loss: 0.4875\n",
      "Step 285 - Loss: 0.359\n",
      "Step 286 - Loss: 0.8093\n",
      "Step 287 - Loss: 0.4372\n",
      "Step 288 - Loss: 0.5686\n",
      "Step 289 - Loss: 0.4775\n",
      "Step 290 - Loss: 0.5514\n",
      "Step 291 - Loss: 0.5473\n",
      "Step 292 - Loss: 0.437\n",
      "Step 293 - Loss: 0.5089\n",
      "Step 294 - Loss: 0.4964\n",
      "Step 295 - Loss: 0.6062\n",
      "Step 296 - Loss: 0.5423\n",
      "Step 297 - Loss: 0.5172\n",
      "Step 298 - Loss: 0.533\n",
      "Step 299 - Loss: 0.5306\n",
      "Step 300 - Loss: 0.5855\n",
      "Step 301 - Loss: 0.3974\n",
      "Step 302 - Loss: 0.3924\n",
      "Step 303 - Loss: 0.4331\n",
      "Step 304 - Loss: 0.4015\n",
      "Step 305 - Loss: 0.532\n",
      "Step 306 - Loss: 0.4486\n",
      "Step 307 - Loss: 0.4023\n",
      "Step 308 - Loss: 0.4099\n",
      "Step 309 - Loss: 0.4798\n",
      "Step 310 - Loss: 0.4019\n",
      "Step 311 - Loss: 0.4358\n",
      "Step 312 - Loss: 0.4358\n",
      "Step 313 - Loss: 0.5123\n",
      "Step 314 - Loss: 0.3878\n",
      "Step 315 - Loss: 0.3616\n",
      "Step 316 - Loss: 0.4726\n",
      "Step 317 - Loss: 0.548\n",
      "Step 318 - Loss: 0.598\n",
      "Step 319 - Loss: 0.5229\n",
      "Step 320 - Loss: 0.4433\n",
      "Step 321 - Loss: 0.5154\n",
      "Step 322 - Loss: 0.4259\n",
      "Step 323 - Loss: 0.3665\n",
      "Step 324 - Loss: 0.4712\n",
      "Step 325 - Loss: 0.4077\n",
      "Step 326 - Loss: 0.5133\n",
      "Step 327 - Loss: 0.3987\n",
      "Step 328 - Loss: 0.5507\n",
      "Step 329 - Loss: 0.5412\n",
      "Step 330 - Loss: 0.5361\n",
      "Step 331 - Loss: 0.4985\n",
      "Step 332 - Loss: 0.5291\n",
      "Step 333 - Loss: 0.4404\n",
      "Step 334 - Loss: 0.4223\n",
      "Step 335 - Loss: 0.5215\n",
      "Step 336 - Loss: 0.4833\n",
      "Step 337 - Loss: 0.4239\n",
      "Step 338 - Loss: 0.5371\n",
      "Step 339 - Loss: 0.4305\n",
      "Step 340 - Loss: 0.5828\n",
      "Step 341 - Loss: 0.4365\n",
      "Step 342 - Loss: 0.4712\n",
      "Step 343 - Loss: 0.4554\n",
      "Step 344 - Loss: 0.5426\n",
      "Step 345 - Loss: 0.5042\n",
      "Step 346 - Loss: 0.5013\n",
      "Step 347 - Loss: 0.5135\n",
      "Step 348 - Loss: 0.5101\n",
      "Step 349 - Loss: 0.5476\n",
      "Step 350 - Loss: 0.5253\n",
      "Step 351 - Loss: 0.3917\n",
      "Step 352 - Loss: 0.3436\n",
      "Step 353 - Loss: 0.3635\n",
      "Step 354 - Loss: 0.4113\n",
      "Step 355 - Loss: 0.4067\n",
      "Step 356 - Loss: 0.4328\n",
      "Step 357 - Loss: 0.5161\n",
      "Step 358 - Loss: 0.4139\n",
      "Step 359 - Loss: 0.3371\n",
      "Step 360 - Loss: 0.4286\n",
      "Step 361 - Loss: 0.4203\n",
      "Step 362 - Loss: 0.4857\n",
      "Step 363 - Loss: 0.4535\n",
      "Step 364 - Loss: 0.3108\n",
      "Step 365 - Loss: 0.3738\n",
      "Step 366 - Loss: 0.4967\n",
      "Step 367 - Loss: 0.4236\n",
      "Step 368 - Loss: 0.4487\n",
      "Step 369 - Loss: 0.4096\n",
      "Step 370 - Loss: 0.4114\n",
      "Step 371 - Loss: 0.4317\n",
      "Step 372 - Loss: 0.5121\n",
      "Step 373 - Loss: 0.3903\n",
      "Step 374 - Loss: 0.4803\n",
      "Step 375 - Loss: 0.4696\n",
      "Step 376 - Loss: 0.4819\n",
      "Step 377 - Loss: 0.4625\n",
      "Step 378 - Loss: 0.5103\n",
      "Step 379 - Loss: 0.4993\n",
      "Step 380 - Loss: 0.4322\n",
      "Step 381 - Loss: 0.4957\n",
      "Step 382 - Loss: 0.4995\n",
      "Step 383 - Loss: 0.5329\n",
      "Step 384 - Loss: 0.467\n",
      "Step 385 - Loss: 0.4227\n",
      "Step 386 - Loss: 0.4394\n",
      "Step 387 - Loss: 0.41\n",
      "Step 388 - Loss: 0.4866\n",
      "Step 389 - Loss: 0.5902\n",
      "Step 390 - Loss: 0.4557\n",
      "Step 391 - Loss: 0.5016\n",
      "Step 392 - Loss: 0.6038\n",
      "Step 393 - Loss: 0.4524\n",
      "Step 394 - Loss: 0.4442\n",
      "Step 395 - Loss: 0.5854\n",
      "Step 396 - Loss: 0.5989\n",
      "Step 397 - Loss: 0.5023\n",
      "Step 398 - Loss: 0.4016\n",
      "Step 399 - Loss: 0.5018\n",
      "Step 400 - Loss: 0.6072\n",
      "Step 401 - Loss: 0.3773\n",
      "Step 402 - Loss: 0.3528\n",
      "Step 403 - Loss: 0.3649\n",
      "Step 404 - Loss: 0.3889\n",
      "Step 405 - Loss: 0.4104\n",
      "Step 406 - Loss: 0.4079\n",
      "Step 407 - Loss: 0.4021\n",
      "Step 408 - Loss: 0.415\n",
      "Step 409 - Loss: 0.5146\n",
      "Step 410 - Loss: 0.4855\n",
      "Step 411 - Loss: 0.6026\n",
      "Step 412 - Loss: 0.3712\n",
      "Step 413 - Loss: 0.3927\n",
      "Step 414 - Loss: 0.3145\n",
      "Step 415 - Loss: 0.383\n",
      "Step 416 - Loss: 0.3901\n",
      "Step 417 - Loss: 0.3772\n",
      "Step 418 - Loss: 0.4227\n",
      "Step 419 - Loss: 0.4487\n",
      "Step 420 - Loss: 0.3442\n",
      "Step 421 - Loss: 0.3847\n",
      "Step 422 - Loss: 0.3985\n",
      "Step 423 - Loss: 0.4335\n",
      "Step 424 - Loss: 0.391\n",
      "Step 425 - Loss: 0.5411\n",
      "Step 426 - Loss: 0.3772\n",
      "Step 427 - Loss: 0.3757\n",
      "Step 428 - Loss: 0.4061\n",
      "Step 429 - Loss: 0.411\n",
      "Step 430 - Loss: 0.4399\n",
      "Step 431 - Loss: 0.4117\n",
      "Step 432 - Loss: 0.4501\n",
      "Step 433 - Loss: 0.4559\n",
      "Step 434 - Loss: 0.3636\n",
      "Step 435 - Loss: 0.5651\n",
      "Step 436 - Loss: 0.3883\n",
      "Step 437 - Loss: 0.4619\n",
      "Step 438 - Loss: 0.4806\n",
      "Step 439 - Loss: 0.3883\n",
      "Step 440 - Loss: 0.3995\n",
      "Step 441 - Loss: 0.4963\n",
      "Step 442 - Loss: 0.529\n",
      "Step 443 - Loss: 0.4493\n",
      "Step 444 - Loss: 0.5821\n",
      "Step 445 - Loss: 0.4248\n",
      "Step 446 - Loss: 0.4857\n",
      "Step 447 - Loss: 0.5406\n",
      "Step 448 - Loss: 0.4793\n",
      "Step 449 - Loss: 0.6249\n",
      "Step 450 - Loss: 0.5794\n",
      "Step 451 - Loss: 0.4304\n",
      "Step 452 - Loss: 0.3448\n",
      "Step 453 - Loss: 0.4306\n",
      "Step 454 - Loss: 0.3752\n",
      "Step 455 - Loss: 0.3661\n",
      "Step 456 - Loss: 0.2937\n",
      "Step 457 - Loss: 0.3705\n",
      "Step 458 - Loss: 0.3533\n",
      "Step 459 - Loss: 0.327\n",
      "Step 460 - Loss: 0.3634\n",
      "Step 461 - Loss: 0.4005\n",
      "Step 462 - Loss: 0.3135\n",
      "Step 463 - Loss: 0.4578\n",
      "Step 464 - Loss: 0.3174\n",
      "Step 465 - Loss: 0.4738\n",
      "Step 466 - Loss: 0.3324\n",
      "Step 467 - Loss: 0.3173\n",
      "Step 468 - Loss: 0.4504\n",
      "Step 469 - Loss: 0.3894\n",
      "Step 470 - Loss: 0.4398\n",
      "Step 471 - Loss: 0.4127\n",
      "Step 472 - Loss: 0.4187\n",
      "Step 473 - Loss: 0.5053\n",
      "Step 474 - Loss: 0.3386\n",
      "Step 475 - Loss: 0.3193\n",
      "Step 476 - Loss: 0.4181\n",
      "Step 477 - Loss: 0.3996\n",
      "Step 478 - Loss: 0.4321\n",
      "Step 479 - Loss: 0.3668\n",
      "Step 480 - Loss: 0.4476\n",
      "Step 481 - Loss: 0.5234\n",
      "Step 482 - Loss: 0.43\n",
      "Step 483 - Loss: 0.3765\n",
      "Step 484 - Loss: 0.3984\n",
      "Step 485 - Loss: 0.4595\n",
      "Step 486 - Loss: 0.4544\n",
      "Step 487 - Loss: 0.4603\n",
      "Step 488 - Loss: 0.4072\n",
      "Step 489 - Loss: 0.4694\n",
      "Step 490 - Loss: 0.3945\n",
      "Step 491 - Loss: 0.4483\n",
      "Step 492 - Loss: 0.4548\n",
      "Step 493 - Loss: 0.4946\n",
      "Step 494 - Loss: 0.7069\n",
      "Step 495 - Loss: 0.4995\n",
      "Step 496 - Loss: 0.4571\n",
      "Step 497 - Loss: 0.4303\n",
      "Step 498 - Loss: 0.4752\n",
      "Step 499 - Loss: 0.4944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500 - Loss: 0.564\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 896.00 MiB. GPU 0 has a total capacity of 23.59 GiB of which 630.44 MiB is free. Including non-PyTorch memory, this process has 22.96 GiB memory in use. Of the allocated memory 20.81 GiB is allocated by PyTorch, and 1.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Cell 2: Then run training\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal training metrics:\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_result\u001b[38;5;241m.\u001b[39mmetrics)\n\u001b[1;32m      6\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./final_control_system_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3_finetuning_env/lib/python3.11/site-packages/transformers/trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2165\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3_finetuning_env/lib/python3.11/site-packages/transformers/trainer.py:2591\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2589\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2590\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2591\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\n\u001b[1;32m   2593\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2594\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2595\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3_finetuning_env/lib/python3.11/site-packages/transformers/trainer.py:3049\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time)\u001b[0m\n\u001b[1;32m   3047\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 3049\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3050\u001b[0m     is_new_best_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_determine_best_metric(metrics\u001b[38;5;241m=\u001b[39mmetrics, trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m   3052\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_strategy \u001b[38;5;241m==\u001b[39m SaveStrategy\u001b[38;5;241m.\u001b[39mBEST:\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3_finetuning_env/lib/python3.11/site-packages/transformers/trainer.py:3003\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   3002\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 3003\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3004\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   3006\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3_finetuning_env/lib/python3.11/site-packages/transformers/trainer.py:4050\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4047\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   4049\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 4050\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4051\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4053\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   4054\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   4055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4058\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4060\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   4061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3_finetuning_env/lib/python3.11/site-packages/transformers/trainer.py:4244\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4241\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[1;32m   4243\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 4244\u001b[0m losses, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4245\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4246\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   4247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4248\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3_finetuning_env/lib/python3.11/site-packages/transformers/trainer.py:4470\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   4468\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 4470\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m   4472\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ignore_keys)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3_finetuning_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3_finetuning_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3_finetuning_env/lib/python3.11/site-packages/accelerate/utils/operations.py:823\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3_finetuning_env/lib/python3.11/site-packages/accelerate/utils/operations.py:811\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 811\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3_finetuning_env/lib/python3.11/site-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3_finetuning_env/lib/python3.11/site-packages/peft/peft_model.py:849\u001b[0m, in \u001b[0;36mPeftModel.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    848\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m--> 849\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3_finetuning_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3_finetuning_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3_finetuning_env/lib/python3.11/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3_finetuning_env/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1163\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m   1160\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1162\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1163\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3_finetuning_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3_finetuning_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3_finetuning_env/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:913\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    901\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    902\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    903\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         position_embeddings,\n\u001b[1;32m    911\u001b[0m     )\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 913\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    925\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3_finetuning_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3_finetuning_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3_finetuning_env/lib/python3.11/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3_finetuning_env/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:656\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    654\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    655\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 656\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    657\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    659\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3_finetuning_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3_finetuning_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3_finetuning_env/lib/python3.11/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3_finetuning_env/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:242\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 242\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 896.00 MiB. GPU 0 has a total capacity of 23.59 GiB of which 630.44 MiB is free. Including non-PyTorch memory, this process has 22.96 GiB memory in use. Of the allocated memory 20.81 GiB is allocated by PyTorch, and 1.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Cell 2: Then run training\n",
    "print(\"Starting training...\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"Final training metrics:\", train_result.metrics)\n",
    "trainer.save_model(\"./final_control_system_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./control_system_tuned_model/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from vllm import LLM, SamplingParams  # vLLM tools for efficient language model inference\n",
    "import os  # For setting environment variables\n",
    "\n",
    "# Configure PyTorch memory allocation\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'  \n",
    "# This sets the maximum size of a memory block that PyTorch will allocate to 512MB, helping prevent memory fragmentation\n",
    "\n",
    "def generate_response(prompt, max_tokens=100):\n",
    "    # Function that takes a prompt and max tokens as input\n",
    "    # max_tokens=50 is the default value if not specified\n",
    "    \n",
    "    # Format the prompt using the chat template\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": prompt}],  # Creates a chat message format\n",
    "        add_generation_prompt=True,  # Adds any necessary generation prompts\n",
    "        tokenize=False,  # Returns string instead of tokens\n",
    "    )\n",
    "    \n",
    "    # Configure generation parameters\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.7,  # Controls randomness (higher = more random)\n",
    "        top_p=0.9,  # Nucleus sampling parameter (higher = more diverse)\n",
    "        max_tokens=max_tokens,  # Maximum number of tokens to generate\n",
    "        presence_penalty=0.0,  # Penalize new tokens based on presence in text\n",
    "        frequency_penalty=0.0  # Penalize new tokens based on frequency in text\n",
    "    )\n",
    "    \n",
    "    # Generate the response with memory-efficient settings\n",
    "    with torch.inference_mode():  # Disables gradient computation to save memory\n",
    "        # Convert input text to tokens and move to same device as model\n",
    "        inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        # Generate output using the model\n",
    "        outputs = model.generate(\n",
    "            **inputs,  # Pass tokenized inputs\n",
    "            max_new_tokens=max_tokens,  # Maximum length of generated text\n",
    "            do_sample=True,  # Enable sampling (vs greedy decoding)\n",
    "            temperature=0.7,  # Controls randomness\n",
    "            top_p=0.9,  # Nucleus sampling parameter\n",
    "            num_beams=1,  # Disable beam search to save memory\n",
    "            pad_token_id=tokenizer.pad_token_id,  # ID for padding token\n",
    "            eos_token_id=tokenizer.eos_token_id,  # ID for end of sequence token\n",
    "            use_cache=True  # Enable KV-caching for faster generation\n",
    "        )\n",
    "    \n",
    "    # Convert output tokens back to text\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "def batch_generate(prompts, max_tokens=100):\n",
    "    # Function to process multiple prompts\n",
    "    responses = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        try:\n",
    "            # Try to generate a response for each prompt\n",
    "            response = generate_response(prompt, max_tokens)\n",
    "            responses.append({\"prompt\": prompt, \"response\": response})\n",
    "        except Exception as e:\n",
    "            # Handle any errors that occur during generation\n",
    "            print(f\"Error processing prompt '{prompt}': {str(e)}\")\n",
    "            responses.append({\"prompt\": prompt, \"response\": \"Error generating response\"})\n",
    "    return responses\n",
    "\n",
    "# Example usage\n",
    "test_prompts = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Explain quantum computing in simple terms\"\n",
    "]\n",
    "\n",
    "# Clear GPU memory before generation\n",
    "torch.cuda.empty_cache()  # Free up any unused GPU memory\n",
    "\n",
    "# Generate responses for all prompts\n",
    "results = batch_generate(test_prompts)\n",
    "\n",
    "# Print results\n",
    "for result in results:\n",
    "    print(f\"Prompt: {result['prompt']}\")  # Print the original prompt\n",
    "    print(f\"Response: {result['response']}\")  # Print the generated response\n",
    "    print(\"-\" * 50)  # Print a separator line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "def process_questions(questions, tokenizer, model, sampling_params):\n",
    "    # Format questions using chat template\n",
    "    questions_processed = [tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": q}],\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    ) for q in questions]\n",
    "    \n",
    "    # Generate answers with memory-efficient settings\n",
    "    answers = []\n",
    "    for prompt in questions_processed:\n",
    "        with torch.inference_mode():\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=sampling_params.max_tokens,\n",
    "                do_sample=sampling_params.do_sample,\n",
    "                temperature=sampling_params.temperature,\n",
    "                top_p=sampling_params.top_p,\n",
    "                num_beams=1,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                use_cache=True\n",
    "            )\n",
    "            answers.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "    \n",
    "    return answers\n",
    "\n",
    "# Define evaluation parameters\n",
    "class SamplingParams:\n",
    "    def __init__(self, n=1, temperature=0, top_p=1, seed=123, max_tokens=2048, stop_token_ids=None):\n",
    "        self.n = n\n",
    "        self.temperature = temperature\n",
    "        self.top_p = top_p\n",
    "        self.seed = seed\n",
    "        self.max_tokens = max_tokens\n",
    "        self.stop_token_ids = stop_token_ids\n",
    "        self.do_sample = temperature > 0\n",
    "\n",
    "# Template for evaluation\n",
    "prompt_template = \"\"\"Given a question, an model-generated answer and a reasoning step from the ground-truth answer. \n",
    "You are required to analyze and tell if the model-generated answer contains the given reasoning step. \n",
    "End your answer with [[Yes]] or [[No]].\n",
    "\n",
    "Question: {}\n",
    "Model-generated answer: {}\n",
    "Reasoning Step: {}\"\"\"\n",
    "\n",
    "def evaluate_answers(questions, solutions, answers, tokenizer, model, eval_params):\n",
    "    dump_data_full = []\n",
    "    \n",
    "    for question, gt_steps, gen_answer in zip(questions, solutions, answers):\n",
    "        dump_data = [{\n",
    "            'question': question,\n",
    "            'answer': gen_answer,\n",
    "            'contamination': []\n",
    "        }]\n",
    "        \n",
    "        prompt_set = []\n",
    "        for gt_step in gt_steps:\n",
    "            prompt_set.append(prompt_template.format(question, gen_answer, gt_step))\n",
    "            \n",
    "        if not prompt_set:\n",
    "            continue\n",
    "            \n",
    "        prompt_set_processed = [tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": p}],\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False,\n",
    "        ) for p in prompt_set]\n",
    "        \n",
    "        # Generate evaluations\n",
    "        eval_results = []\n",
    "        for prompt in prompt_set_processed:\n",
    "            with torch.inference_mode():\n",
    "                inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "                output = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=eval_params.max_tokens,\n",
    "                    do_sample=eval_params.do_sample,\n",
    "                    temperature=eval_params.temperature,\n",
    "                    top_p=eval_params.top_p,\n",
    "                    num_beams=1,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                    use_cache=True\n",
    "                )\n",
    "                eval_results.append(tokenizer.decode(output[0], skip_special_tokens=True).strip())\n",
    "        \n",
    "        # Process evaluation results\n",
    "        for i, eval_result in enumerate(eval_results):\n",
    "            if eval_result[-1] == '.':\n",
    "                eval_result = eval_result[:-1]\n",
    "                \n",
    "            contain = None\n",
    "            if '[[Yes]]' in eval_result:\n",
    "                contain = True\n",
    "            elif '[[No]]' in eval_result:\n",
    "                contain = False\n",
    "                \n",
    "            dump_data[0]['contamination'].append({\n",
    "                'gt_step': gt_steps[i],\n",
    "                'is_contain': contain,\n",
    "                'analysis': eval_result\n",
    "            })\n",
    "            \n",
    "        dump_data_full.extend(dump_data)\n",
    "    \n",
    "    return dump_data_full\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    # Your existing model setup code here\n",
    "    \n",
    "    # Setup parameters\n",
    "    sampling_params = SamplingParams(n=1, temperature=0.7, top_p=0.9, max_tokens=2048)\n",
    "    eval_params = SamplingParams(n=1, temperature=0, top_p=1, seed=123, max_tokens=2048, \n",
    "                               stop_token_ids=[128009])  # for llama 3\n",
    "    \n",
    "    # Process questions and generate answers\n",
    "    answers = process_questions(questions, tokenizer, model, sampling_params)\n",
    "    \n",
    "    # Print first input-output example\n",
    "    print(f\"Prompt: {questions[0]!r}\")\n",
    "    print(f\"Generated text: {answers[0]!r}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Evaluate answers\n",
    "    dump_data_full = evaluate_answers(questions, solutions, answers, tokenizer, model, eval_params)\n",
    "    \n",
    "    # Save results\n",
    "    print(f\"Total processed items: {len(dump_data_full)}\")\n",
    "    with open('dump.json', 'w') as f:\n",
    "        json.dump(dump_data_full, f, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3_finetuning_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
